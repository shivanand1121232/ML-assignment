{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"mount_file_id":"1v8l8idgcNxxnBbbWm6QHyI2E5exLFkeE","authorship_tag":"ABX9TyOI+Tfj9O6G+NeCAZrWk9TQ"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["1.)What is a parameter?"],"metadata":{"id":"nNGkTCwwHRU5"}},{"cell_type":"markdown","source":["A parameter is a value or a variable that influences the behavior or outcome of a function, system, or model. It typically serves as an input or setting that defines or limits the operation of a process or calculation. Parameters can be used in various contexts, including mathematics, programming, and science."],"metadata":{"id":"IDEceKdUHZqF"}},{"cell_type":"markdown","source":["2.)What is correlation?"],"metadata":{"id":"rB5toJHuHxfc"}},{"cell_type":"markdown","source":["Correlation is a statistical measure that describes the strength and direction of a relationship between two (or more) variables. When two variables are correlated, changes in one variable tend to be associated with changes in the other. Correlation does not imply causation, meaning that just because two variables are correlated, it doesn't mean that one causes the other to change."],"metadata":{"id":"apZelKv1H2qc"}},{"cell_type":"markdown","source":["3.)What does negative correlation mean?"],"metadata":{"id":"X1ztqaDzH_vr"}},{"cell_type":"markdown","source":["A negative correlation between two variables means that as one variable increases, the other variable tends to decrease, and vice versa. In other words, there is an inverse relationship between the two variables. When one goes up, the other goes down."],"metadata":{"id":"BUbvtna2IJFr"}},{"cell_type":"markdown","source":["4.)Define Machine Learning. What are the main components in Machine Learning?"],"metadata":{"id":"jNkhm__EIOuY"}},{"cell_type":"markdown","source":["Machine Learning (ML) is a subfield of artificial intelligence (AI) that focuses on enabling computers to learn from data and make decisions or predictions without being explicitly programmed. Instead of using predefined rules, machine learning algorithms identify patterns in data, improve their performance over time, and make predictions or decisions based on that data.\n","\n","In simple terms, machine learning allows computers to learn from examples and experience, enabling them to perform tasks that traditionally required human intervention.\n","\n","Main Components in Machine Learning\n","The main components of a machine learning system typically include the following:\n","\n","Data:\n","\n","Data is the foundation of any machine learning model. It includes the input (features) and the target (labels or outcomes) that the model will learn from.\n","Data can come in many forms: numerical, categorical, text, images, audio, etc.\n","Data preprocessing is a crucial step that involves cleaning, transforming, and organizing raw data into a suitable format for model training.\n","Features:\n","\n","Features are individual measurable properties or characteristics of the data. They are used as input for training the machine learning model.\n","For example, in a dataset of house prices, features might include the number of bedrooms, square footage, and location.\n","Feature Engineering involves selecting, modifying, or creating new features that improve model performance.\n","Model:\n","\n","The model represents the mathematical or algorithmic structure that learns patterns from the data. It's the core of any machine learning algorithm.\n","Examples of ML models include:\n","Linear Regression, Decision Trees, Support Vector Machines (SVM), Neural Networks, etc.\n","The model's purpose is to make predictions based on input data (e.g., classifying an email as spam or not, predicting future sales).\n","Training:\n","\n","Training is the process where the machine learning model learns from the provided training data. During training, the algorithm adjusts internal parameters (such as weights in a neural network) to minimize errors or improve prediction accuracy.\n","The model is optimized using an optimization algorithm (e.g., Gradient Descent) to reduce the difference between the model's predictions and the actual results.\n","Loss Function (or Objective Function):\n","\n","The loss function measures the error or difference between the model's predictions and the actual target values. The goal is to minimize this error during training.\n","For example, in regression tasks, the loss function might be Mean Squared Error (MSE), while in classification tasks, it could be Cross-Entropy Loss.\n","Learning Algorithm:\n","\n","The learning algorithm defines how the model updates its parameters based on the training data. It dictates how the model learns from the data and adjusts itself over time.\n","Common algorithms include:\n","Gradient Descent (used in neural networks, linear regression, etc.)\n","Random Forest (an ensemble method)\n","K-Means (used in clustering)\n","Q-Learning (used in reinforcement learning)"],"metadata":{"id":"yYK0Am6JIV_l"}},{"cell_type":"markdown","source":["5.)How does loss value help in determining whether the model is good or not?"],"metadata":{"id":"HZlvYe7eI0Mr"}},{"cell_type":"markdown","source":["In machine learning, the loss value (or loss function) is a crucial metric used to assess how well a model is performing. It quantifies the difference between the model's predictions and the actual outcomes (or ground truth), helping to determine how far off the model's predictions are from the desired results.\n","\n","How Loss Helps Determine Model Performance:\n","Quantifying Model Error:\n","\n","Optimization and Model Improvement:\n","\n","Model Evaluation:\n","\n"],"metadata":{"id":"IR5dLHjWJRSc"}},{"cell_type":"markdown","source":["6).What are continuous and categorical variables?"],"metadata":{"id":"aOj_YX9LJ0dj"}},{"cell_type":"markdown","source":[". Continuous Variables:\n","Continuous variables, also known as quantitative variables, represent measurable quantities that can take an infinite number of values within a given range. These values are typically numeric and can be expressed with precision (often with decimals or fractions).\n","\n","\n","Categorical variables, also known as qualitative variables, represent distinct categories or groups. These values are typically non-numeric and describe characteristics or attributes that do not have a natural order or scale (in some cases, they can be ordered)."],"metadata":{"id":"0QW-BjEfJ1ni"}},{"cell_type":"markdown","source":["7.)How do we handle categorical variables in Machine Learning? What are the common t\n","echniques?\n"],"metadata":{"id":"v1I4U9j3KizM"}},{"cell_type":"markdown","source":["Handling categorical variables is an essential step in the machine learning pipeline because most machine learning algorithms require numerical input. Categorical variables represent discrete categories or groups (such as gender, color, or product type), which need to be converted into numerical representations before they can be used for model training.\n","\n"," 1.Label Encoding\n"," 2.One-Hot Encoding\n"," 3.Binary Encoding\n"," 4.Target Encoding (Mean Encoding)\n"," 5.Frequency Encoding\n"],"metadata":{"id":"KjgLhl6GKwy8"}},{"cell_type":"markdown","source":["8.)What do you mean by training and testing a dataset?"],"metadata":{"id":"scjLper5LszE"}},{"cell_type":"markdown","source":["1. Training a Dataset\n","Definition: Training a dataset involves using a portion of the data to \"teach\" a machine learning model to recognize patterns and make predictions or decisions.\n","How it works:\n","The model is provided with input data (features) and corresponding correct answers (labels or target values).\n","It adjusts its internal parameters (e.g., weights in neural networks) to minimize errors in predicting the labels.\n","The goal is for the model to generalize the relationships in the data so it can make accurate predictions on unseen data.\n","Example:\n","In a dataset predicting house prices, training data would include features like size, location, and the actual price.\n","The model learns the relationship between these features and the price.\n","2. Testing a Dataset\n","Definition: Testing a dataset is used to evaluate the model's performance on new, unseen data that was not used during training.\n","How it works:\n","After the model is trained, it is fed the testing dataset, which also contains inputs and their true labels.\n","The model makes predictions, which are then compared to the true labels.\n","Performance metrics (e.g., accuracy, precision, recall, mean squared error) are calculated to assess how well the model has learned and generalized.\n","Example:\n","Using the house price model, the testing data would include unseen house features and their prices.\n","The model predicts prices for the testing data, and those predictions are compared with the actual prices to compute an error rate.\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n"],"metadata":{"id":"B6UDEVxXL7tV"}},{"cell_type":"markdown","source":["9.)What is sklearn.preprocessing?"],"metadata":{"id":"925EhJHIMNW9"}},{"cell_type":"markdown","source":["sklearn.preprocessing is a module in Scikit-learn, a popular Python library for machine learning. This module provides tools for preparing and transforming data into a format that is optimal for machine learning models. Data preprocessing is a crucial step because most algorithms require data to meet certain conditions, such as having numerical values, being scaled appropriately, or normalized."],"metadata":{"id":"CZeUTwOjMXYJ"}},{"cell_type":"markdown","source":["10.)What is a Test set?"],"metadata":{"id":"H0z5ftD9McRu"}},{"cell_type":"markdown","source":["A test set is a subset of data used to evaluate the performance of a machine learning model after it has been trained on the training set. The purpose of the test set is to simulate how the model will perform on unseen data, ensuring that it generalizes well beyond the training data.\n","\n"],"metadata":{"id":"5T21BzTrMi72"}},{"cell_type":"markdown","source":["11.)How do we split data for model fitting (training and testing) in Python?\n","How do you approach a Machine Learning problem?"],"metadata":{"id":"DL4z5QbRMtJe"}},{"cell_type":"markdown","source":["1. Splitting Data for Model Fitting (Training and Testing) in Python\n","To split data into training and testing sets, you can use the train_test_split function from Scikit-learn. Here's how:\n","\n","Basic Example\n","from sklearn.model_selection import train_test_split\n","import numpy as np\n","\n","# Example data\n","X = np.random.rand(100, 5)  # Features (100 samples, 5 features)\n","y = np.random.randint(0, 2, 100)  # Target labels (binary classification)\n","\n","# Split data into training and test sets (80% training, 20% testing)\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n","\n","# Outputs\n","print(\"Training set size:\", X_train.shape[0])\n","print(\"Test set size:\", X_test.shape[0])\n","\n","\n","2. Approach to a Machine Learning Problem\n","Here’s a systematic framework for tackling any machine learning problem:\n","\n","step 1: Understand the Problem\n","Step 2: Exploratory Data Analysis (EDA\n","Step 3: Data Preprocessing\n","Step 4: Split Data\n","Step 5: Choose a Model\n","Step 6: Train the Model\n","Step 7: Evaluate the Model\n","Step 8: Improve the Model\n","Step 9: Deploy and Monitor"],"metadata":{"id":"XwP5Z0OXM4wq"}},{"cell_type":"markdown","source":["12.)Why do we have to perform EDA before fitting a model to the data"],"metadata":{"id":"cF03v4gxODIF"}},{"cell_type":"markdown","source":["Performing Exploratory Data Analysis (EDA) before fitting a machine learning model is a crucial step because it helps you understand the data, detect potential issues, and make informed decisions about preprocessing, feature engineering, and model selection. Here are the key reasons why EDA is essentia\n","1. Understand the Data\n","2. Detect Data Issues\n","3. Evaluate Feature Relevance\n","4. Inform Data Preprocessing Steps\n","5. Avoid Garbage In, Garbage Out\n","6. Guide Model Selection\n","7. Improve Communication"],"metadata":{"id":"Nw8nniJFOQB8"}},{"cell_type":"markdown","source":["13.)How can you find correlation between variables in Python?"],"metadata":{"id":"-0PKo03GPvaE"}},{"cell_type":"markdown","source":["In Python, you can compute the correlation between variables using pandas, numpy, or scipy. The most common method is to use pandas for data stored in a DataFrame. Correlation measures the statistical relationship between two variables, with values ranging from -1 to 1:\n","\n","+1: Perfect positive correlation (as one variable increases, the other increases).\n","-1: Perfect negative correlation (as one variable increases, the other decreases).\n","0: No correlation."],"metadata":{"id":"MpDRepdmP1oy"}},{"cell_type":"markdown","source":["14.)What is causation? Explain difference between correlation and causation with an example."],"metadata":{"id":"zEjkX4_pQD8Y"}},{"cell_type":"markdown","source":["Causation refers to a relationship where one variable directly influences or causes a change in another variable. It implies a cause-and-effect relationship, meaning that changes in one variable are responsible for changes in another.\n","\n","Difference Between Correlation and Causation\n","Aspect\tCorrelation\tCausation\n","Definition\tA statistical relationship or association between two variables.\tA direct cause-and-effect relationship between two variables.\n","Directionality\tDoes not imply which variable influences the other.\tImplies that one variable causes changes in another.\n","Dependence\tCan exist without a causal relationship.\tAlways involves correlation but adds a causal link.\n","Proof\tObservational.\tRequires controlled experiments or strong evidence.\n","Examples\n","1. Correlation Without Causation\n","Observation: Ice cream sales and drowning incidents are positively correlated.\n","Explanation:\n","Both increase during summer, but ice cream sales do not cause drowning.\n","The underlying third variable (hot weather) influences both.\n","2. Causation Example\n","Observation: Smoking increases the risk of lung cancer.\n","Explanation:\n","Experimental and epidemiological studies show that chemicals in tobacco directly damage DNA, leading to cancer.\n","How to Identify Causation\n","Controlled Experiments: The gold standard for proving causation (e.g., clinical trials).\n","Temporal Relationship: Cause must precede the effect.\n","Eliminating Confounders: Account for external factors that could explain the relationship.\n","Consistency Across Studies: Causation is more likely if multiple studies find the same result under different conditions.\n","Illustration of Correlation vs. Causation\n","Example:\n","Scenario: A study finds that students who take music lessons have higher math scores.\n","Interpretation:\n","Correlation: Music lessons and math scores are associated.\n","Causation (false assumption): Music lessons cause better math scores.\n","Reality: A third variable (e.g., family income or parental involvement) might explain both.\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n"],"metadata":{"id":"BU_klhL8QJbA"}},{"cell_type":"markdown","source":["15.)What is an Optimizer? What are different types of optimizers? Explain each with an example"],"metadata":{"id":"LtMeOMdSQ4k8"}},{"cell_type":"markdown","source":["An optimizer is an algorithm or method used in machine learning and deep learning to adjust the parameters of a model (e.g., weights and biases) to minimize the loss function, which measures the error between the predicted outputs and the actual targets. Optimizers play a crucial role in training neural networks by determining how the model updates its parameters to improve performance.\n","1.Gradient Descent (GD)\n","2.Stochastic Gradient Descent (SGD)\n","Momentum\n","4.Adagrad (Adaptive Gradient Algorithm)\n","5. RMSProp (Root Mean Square Propagation)\n","6. Adam (Adaptive Moment Estimation)\n","7. Nadam (Nesterov-accelerated Adaptive Moment Estimation)\n"],"metadata":{"id":"9bDY2gJRS9VX"}},{"cell_type":"markdown","source":["16.)What is sklearn.linear_model ?"],"metadata":{"id":"ljd1YzqWF83y"}},{"cell_type":"markdown","source":["sklearn.linear_model is a module in scikit-learn, a popular machine learning library in Python. This module provides a collection of algorithms for linear models, which are used for regression and classification tasks. These models predict outcomes based on a linear relationship between input features and the target variable."],"metadata":{"id":"eJkZboxRGCcv"}},{"cell_type":"markdown","source":["The model.fit() method in scikit-learn is used to train a machine learning model. It adjusts the internal parameters (e.g., weights and biases for linear models) of the model to minimize the error or optimize the performance metric, based on the provided training data.\n","\n","What Does model.fit() Do?\n","1.)Accepts Training Data: It takes input features (\n","𝑋\n","X) and target labels (\n","𝑦\n","y) as arguments.\n","2.)Computes Gradients (if applicable): For models like linear regression or neural networks, it calculates gradients to optimize parameters.\n","3.)Optimizes Parameters: Updates the model's internal parameters using algorithms specific to the model (e.g., gradient descent for linear regression, probability-based estimation for logistic regression).\n","4.)Validates Inputs: Ensures that the input data (\n","𝑋\n",",\n","𝑦\n","X,y) has the correct format and dimensions.\n","5.)Stores Information: Saves the learned parameters or other training-related information in the model object."],"metadata":{"id":"hrKK-q18GJO8"}},{"cell_type":"markdown","source":["17.)What does model.predict() do? What arguments must be given?"],"metadata":{"id":"WUpsZzBTE8lr"}},{"cell_type":"markdown","source":["The model.predict() method in scikit-learn is used to make predictions on new or unseen data after a machine learning model has been trained using the fit() metho\n","\n","  What Does model.predict() Do?\n","Accepts Input Data: It takes a feature matrix\n","𝑋\n","X (the same structure as the training data) as an argument.\n","2.)Applies the Trained Model: Uses the parameters (e.g., weights, biases) learned during training to compute predictions for the input data.\n","3.)Returns Predictions: Outputs the predicted values or class labels, depending on the type of model.\n"],"metadata":{"id":"YNSlHSj9IAf2"}},{"cell_type":"markdown","source":["18.)What are continuous and categorical variables?"],"metadata":{"id":"e6eOunQfIiD0"}},{"cell_type":"markdown","source":["1. Continuous Variables\n","Definition:\n","Continuous variables represent data that can take on any value within a range. These values are numerical and are usually measured on a scale or continuum. They can include fractions and decimals.\n","\n","\n"," 2. Categorical Variables\n","Definition:\n","Categorical variables represent data that can be divided into distinct groups or categories. These values are qualitative and usually represent labels or categories."],"metadata":{"id":"nTZdwV3aIjLw"}},{"cell_type":"markdown","source":["19.)What is feature scaling? How does it help in Machine Learning?"],"metadata":{"id":"vknVukwoIwUa"}},{"cell_type":"markdown","source":["   Feature Scaling?\n","Feature scaling is a data preprocessing technique used to standardize or normalize the range of independent variables (features) in a dataset. It ensures that all features have comparable scales, especially when they are measured in different units (e.g., height in centimeters and weight in kilograms).\n","    Why Feature Scaling is Important in Machine Learning?\n","Feature scaling is crucial for many machine learning algorithms that compute distances, gradients, or weights, as these computations can be disproportionately affected by the range of feature values. Proper scaling ensures that all features contribute equally to the model.\n","\n"," How Feature Scaling Helps:\n","1.)Improves Model Performance:\n","\n","Some algorithms (e.g., Gradient Descent) perform better when the feature ranges are uniform, as scaling helps converge faster during optimization.\n","\n","2.)Equal Feature Contribution:\n","\n","Without scaling, features with larger ranges might dominate the objective function, overshadowing features with smaller ranges.\n","\n","3.)Enhances Distance-Based Algorithms:\n","\n","In algorithms like K-Nearest Neighbors (KNN) or Support Vector Machines (SVM), unscaled features can distort distance metrics, leading to suboptimal performance.\n","\n","4.)Regularization:\n","\n","Regularization techniques (e.g., L1, L2 regularization) are sensitive to feature magnitude, so scaling ensures uniform penalization.\n","    "],"metadata":{"id":"cH0u10yzI4bk"}},{"cell_type":"markdown","source":["20.)How do we perform scaling in Python?"],"metadata":{"id":"nMuGg2DqJdhv"}},{"cell_type":"markdown","source":["  In Python, scaling can be performed using preprocessing tools provided by libraries such as scikit-learn. These tools include classes like StandardScaler, MinMaxScaler, RobustScaler, and others for different scaling techniques.\n","  1.)Standardization (Z-Score Scaling)\n","    from sklearn.preprocessing import StandardScaler\n","import numpy as np\n","\n","# Example dataset\n","data = np.array([[1, 200], [2, 300], [3, 400]])\n","\n","# Initialize scaler\n","scaler = StandardScaler()\n","\n","# Fit and transform the data\n","scaled_data = scaler.fit_transform(data)\n","\n","print(\"Standardized Data:\\n\", scaled_data)\n"],"metadata":{"id":"xFAR_nRjJkX5"}},{"cell_type":"markdown","source":["21.)What is sklearn.preprocessing?"],"metadata":{"id":"3aGyysIIKCXN"}},{"cell_type":"markdown","source":["sklearn.preprocessing is a module in scikit-learn that provides tools for data preprocessing. These tools are used to transform raw data into a suitable format for machine learning models. It includes methods for feature scaling, normalization, encoding categorical variables, and generating polynomial features, among other utilities.\n","\n","Data preprocessing is a critical step in the machine learning pipeline, as it ensures that the data is clean, standardized, and ready for model training.\n","\n"],"metadata":{"id":"TQQ7_Pz-KD1F"}},{"cell_type":"markdown","source":["22.)How do we split data for model fitting (training and testing) in Python?"],"metadata":{"id":"eq4OPMNAKOuL"}},{"cell_type":"markdown","source":["Splitting data into training and testing sets is a fundamental step in preparing your data for machine learning. This ensures that the model is evaluated on unseen data, simulating real-world scenarios and helping to avoid overfitting.\n","\n","In Python, the most common way to split data is using the train_test_split function from scikit-learn.\n","\n","How to Use train_test_split\n","\n","  from sklearn.model_selection import train_test_split\n","\n","X_train, X_test, y_train, y_test = train_test_split(\n","    X, y, test_size=0.2, random_state=42, stratify=y\n",")\n"],"metadata":{"id":"2GRx_oB0Kb7A"}},{"cell_type":"markdown","source":["23.)Explain data encoding?"],"metadata":{"id":"F3kfoJkNKyT7"}},{"cell_type":"markdown","source":["Data encoding is the process of converting categorical data (non-numeric data) into a numeric format that can be used by machine learning algorithms. Most machine learning models work with numerical data, and categorical features must be transformed to ensure compatibility and meaningful interpretation by the model."],"metadata":{"id":"gy8YxmBoK2wZ"}},{"cell_type":"markdown","source":[],"metadata":{"id":"AQVjLbllK9T8"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"261C3hlVHMme"},"outputs":[],"source":[]}]}